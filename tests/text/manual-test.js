#!/usr/bin/env node

/**
 * æ‰‹å‹•æ¸¬è©¦è…³æœ¬
 * ç”¨æ–¼å¿«é€Ÿé©—è­‰ tokenize åŠŸèƒ½
 */

import { normalizeIngredients } from "../../lib/text/normalize.js";
import { tokenizeWithECodes } from "../../lib/text/tokenize.js";

function test(name, input, expectedTokens) {
  console.log(`\nğŸ§ª Test: ${name}`);
  console.log(`Input: ${input}`);

  const normalized = normalizeIngredients(input);
  const result = tokenizeWithECodes(normalized);

  console.log(`Tokens: [${result.tokens.join(", ")}]`);

  let pass = true;
  for (const expected of expectedTokens) {
    if (!result.tokens.includes(expected)) {
      console.log(`âŒ Missing: ${expected}`);
      pass = false;
    }
  }

  if (pass) {
    console.log("âœ… PASS");
  } else {
    console.log("âŒ FAIL");
  }

  return pass;
}

console.log("=".repeat(60));
console.log("Tokenize Separator Tests");
console.log("=".repeat(60));

let allPass = true;

allPass &= test("å¥è™Ÿåˆ†éš”", "æ‰‡è²å”‡.ç ‚ç³–.é£Ÿé¹½.é‚„åŸæ°´é£´.é†¬æ²¹.é‡€é€ é†‹", [
  "æ‰‡è²å”‡",
  "ç ‚ç³–",
  "é£Ÿé¹½",
  "é‚„åŸæ°´é£´",
  "é†¬æ²¹",
  "é‡€é€ é†‹",
]);

allPass &= test("åŒ–å­¸åç¨±é€£å­—è™Ÿ", "D-å±±æ¢¨é†‡æ¶².L-éº©é…¸éˆ‰.DL-è˜‹æœé…¸", ["d-å±±æ¢¨é†‡æ¶²", "l-éº©é…¸éˆ‰", "dl-è˜‹æœé…¸"]);

allPass &= test("é€—è™Ÿåˆ†éš”", "æ°´,ç³–,é¹½,æ²¹", ["æ°´", "ç³–", "é¹½", "æ²¹"]);

allPass &= test("é “è™Ÿåˆ†éš”", "æ°´ã€ç³–ã€é¹½ã€æ²¹", ["æ°´", "ç³–", "é¹½", "æ²¹"]);

allPass &= test("æ··åˆåˆ†éš”ç¬¦è™Ÿ", "æ°´.ç³–,é¹½ã€æ²¹;é†‹", ["æ°´", "ç³–", "é¹½", "æ²¹", "é†‹"]);

console.log("\n" + "=".repeat(60));
if (allPass) {
  console.log("âœ… All tests passed!");
} else {
  console.log("âŒ Some tests failed");
  process.exit(1);
}
